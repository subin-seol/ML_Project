{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The University of Melbourne, School of Computing and Information Systems\n",
    "# COMP30027 Machine Learning, 2022 Semester 1\n",
    "\n",
    "## Assignment 2: Sentiment Classification of Tweets\n",
    "\n",
    "This is a sample code to assist you with vectorising the 'Train' dataset for your assignment 2.\n",
    "\n",
    "First we read the CSV datafiles (Train and Test)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "train_data = pd.read_csv(\"Train.csv\", sep=',')\n",
    "test_data = pd.read_csv(\"Test.csv\", sep=',')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>neutral</th>\n",
       "      <td>12659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>positive</th>\n",
       "      <td>5428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>negative</th>\n",
       "      <td>3715</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          count\n",
       "neutral   12659\n",
       "positive   5428\n",
       "negative   3715"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# imbalance of the labels\n",
    "train_labels = sns.countplot(x=\"sentiment\", data=train_data)\n",
    "\n",
    "d = {'count': train_data['sentiment'].value_counts()}\n",
    "pd.DataFrame(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import re\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "sw = stopwords.words('english')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess(text):\n",
    "    \n",
    "    text = text.lower()\n",
    "    # replace everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
    "    text = re.sub(r\"[^a-zA-Z?.!,Â¿]+\", \" \", text)\n",
    "    # removing URLs\n",
    "    text = re.sub(r\"http\\S+\", \"\",text) \n",
    "    \n",
    "    html = re.compile(r'<.*?>') \n",
    "    \n",
    "    # remove html tags\n",
    "    text = html.sub(r'',text) \n",
    "    \n",
    "    punctuations = '@#!?+&*[]-%.:/();$=><|{}^' + \"'`\" + '_'\n",
    "    for p in punctuations:\n",
    "        # remove punctuations\n",
    "        text = text.replace(p,'') \n",
    "        \n",
    "    text = [word.lower() for word in text.split() if word.lower() not in sw]\n",
    "    \n",
    "    # lemmatize verbs\n",
    "    text = [lemmatizer.lemmatize(word, 'v') for word in text]\n",
    "    # lemmatize nouns\n",
    "    text = [lemmatizer.lemmatize(word, 'n') for word in text]\n",
    "    \n",
    "    # remove stopwords\n",
    "    text = \" \".join(text) \n",
    "    \n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    # remove emojis\n",
    "    text = emoji_pattern.sub(r'', text) \n",
    "    \n",
    "    return text\n",
    "\n",
    "train_data['text'] = train_data['text'].apply(lambda x: preprocess(x))\n",
    "test_data['text'] = test_data['text'].apply(lambda x: preprocess(x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we separate the tweet text and the label (sentiment). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train length: 21802\n",
      "Test length: 6099\n"
     ]
    }
   ],
   "source": [
    "#separating instance and label for Train\n",
    "X_train_raw = [x[0] for x in train_data[['text']].values]\n",
    "Y_train = [x[0] for x in train_data[['sentiment']].values]\n",
    "\n",
    "\n",
    "#check the result\n",
    "print(\"Train length:\",len(X_train_raw))\n",
    "\n",
    "#separating instance and label for Test\n",
    "X_test_raw = [x[0] for x in test_data[['text']].values]\n",
    "\n",
    "#check the result\n",
    "print(\"Test length:\", len(X_test_raw))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anybody go radio station tomorrow see shawn friend may go would like make new friend meet\n"
     ]
    }
   ],
   "source": [
    "#Let's see one example tweet\n",
    "print(X_train_raw[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 90:10 split the train data into train & holdout validation set for sampling a large data \n",
    "X_train, X_dev, y_train, y_dev = train_test_split(X_train_raw, Y_train, test_size=0.1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. TFIDF\n",
    "In this approach, we use the **TfidfVectorizer** library to separate all the words in this corpus (dataset). Same as the BoW approach, these words are then used as the 'vectors' or 'features' to represent each instance (Tweet).\n",
    "\n",
    "However, in this method for each instance the value associated with each 'vector' (word) is not the number of times the word repeated in that tweet, but the TFIDF value of then 'voctor' (word)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train feature space size (using TFIDF): (19621, 7260)\n",
      "Dev feature space size (using TFIDF): (2181, 7260)\n",
      "Test feature space size (using TFIDF): (6099, 7260)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words = 'english', min_df = 5, max_df = 0.8, ngram_range=(1, 3))\n",
    "\n",
    "# build the feature set (vocabulary) and vectorise the Train dataset using TFIDF\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "\n",
    "# use the feature set (vocabulary) from Train to vectorise the Dev and Test dataset \n",
    "X_dev_tfidf = tfidf_vectorizer.transform(X_dev)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test_raw)\n",
    "\n",
    "print(\"Train feature space size (using TFIDF):\",X_train_tfidf.shape)\n",
    "print(\"Dev feature space size (using TFIDF):\",X_dev_tfidf.shape)\n",
    "print(\"Test feature space size (using TFIDF):\",X_test_tfidf.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 7142)\t0.3218170628240632\n",
      "  (0, 726)\t0.2428483356556669\n",
      "  (0, 4347)\t0.35640828698792015\n",
      "  (0, 4939)\t0.23117128230527378\n",
      "  (0, 7134)\t0.2370673938941343\n",
      "  (0, 7144)\t0.35640828698792015\n",
      "  (0, 3537)\t0.21772420740033518\n",
      "  (0, 102)\t0.2904063790780171\n",
      "  (0, 3436)\t0.24231244937844257\n",
      "  (0, 725)\t0.24178338986383024\n",
      "  (0, 4008)\t0.348836361471318\n",
      "  (0, 4955)\t0.19082032506707522\n",
      "  (0, 4345)\t0.258360579594547\n"
     ]
    }
   ],
   "source": [
    "#Let's see one example tweet using the TFIDF feature space\n",
    "print(X_train_tfidf[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.7022577850262474\n",
      "Holdout Accuracy: 0.6501604768454837\n",
      "['neutral' 'positive' 'neutral' ... 'negative' 'neutral' 'neutral']\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.61      0.20      0.31       354\n",
      "     neutral       0.65      0.90      0.75      1273\n",
      "    positive       0.69      0.35      0.47       554\n",
      "\n",
      "    accuracy                           0.65      2181\n",
      "   macro avg       0.65      0.49      0.51      2181\n",
      "weighted avg       0.65      0.65      0.61      2181\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "\n",
    "mnb = MultinomialNB()\n",
    "mnb.fit(X_train_tfidf, y_train)\n",
    "\n",
    "print(\"Training Accuracy:\", mnb.score(X_train_tfidf, y_train))\n",
    "print(\"Holdout Accuracy:\", mnb.score(X_dev_tfidf, y_dev))\n",
    "mnb_y_pred = mnb.predict(X_dev_tfidf)\n",
    "\n",
    "print(mnb_y_pred)\n",
    "print(classification_report(y_dev, mnb_y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stratified 5-fold Cross Validation Averaged Accuracy: 0.6335651279375785\n",
      "[0.64618207 0.63402889 0.64013761 0.61995413 0.62752294]\n"
     ]
    }
   ],
   "source": [
    "# 5-fold cross validation\n",
    "# pipeline multinomial\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "folds = StratifiedKFold(n_splits=5)\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('TfidfVectorizer', TfidfVectorizer(stop_words='english', min_df=5, max_df=0.8, ngram_range=(1, 3))),\n",
    "    ('Chi',  SelectKBest(chi2, k=1500)),\n",
    "    ('Multinomial', MultinomialNB())])\n",
    "\n",
    "scores = cross_val_score(pipeline, X_train_raw, Y_train, cv=folds)\n",
    "\n",
    "print(\"Stratified 5-fold CV Mean Accuracy:\", scores.mean())\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=SVC(),\n",
       "             param_grid={'C': [0.01, 0.05, 0.1, 0.5, 1.0, 5.0, 10.0],\n",
       "                         'kernel': ['linear']})"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# gridsearch for the best lin SVC parameters \n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# defining parameter range\n",
    "grid = {'C': [0.01, 0.05, 0.1, 0.5, 1.0, 5.0, 10.0],\n",
    "              'kernel': ['linear']}\n",
    " \n",
    "svm_search = GridSearchCV(SVC(), grid)\n",
    "\n",
    "# fitting the model for grid search\n",
    "svm_search.fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 1.0, 'kernel': 'linear'}\n",
      "SVC(kernel='linear')\n",
      "accuracy : 0.6593443580903406\n"
     ]
    }
   ],
   "source": [
    "# print best parameter after tuning\n",
    "print(svm_search.best_params_)\n",
    " \n",
    "# print how our model looks after hyper-parameter tuning\n",
    "print(svm_search.best_estimator_)\n",
    "print(\"accuracy :\", svm_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.8230467356403853\n",
      "Holdout Accuracy: 0.657496561210454\n",
      "['negative' 'positive' 'neutral' ... 'negative' 'negative' 'neutral']\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.52      0.39      0.45       354\n",
      "     neutral       0.69      0.79      0.74      1273\n",
      "    positive       0.62      0.52      0.57       554\n",
      "\n",
      "    accuracy                           0.66      2181\n",
      "   macro avg       0.61      0.57      0.58      2181\n",
      "weighted avg       0.65      0.66      0.65      2181\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "linsvm = LinearSVC(C=1)\n",
    "linsvm.fit(X_train_tfidf, y_train)\n",
    "\n",
    "print(\"Training Accuracy:\", linsvm.score(X_train_tfidf, y_train))\n",
    "print(\"Holdout Accuracy:\", linsvm.score(X_dev_tfidf, y_dev))\n",
    "linsvm_y_pred = linsvm.predict(X_dev_tfidf)\n",
    "\n",
    "print(linsvm_y_pred)\n",
    "print(classification_report(y_dev, linsvm_y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stratified 5-fold Cross Validation Averaged Accuracy: 0.6604432637914459\n",
      "[0.67507452 0.66177482 0.67522936 0.63899083 0.65114679]\n"
     ]
    }
   ],
   "source": [
    "# 5-fold cross validation\n",
    "# pipeline lin SVM\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('TfidfVectorizer', TfidfVectorizer(stop_words='english', min_df=5, max_df=0.8, ngram_range=(1, 3))),\n",
    "    ('Chi',  SelectKBest(chi2, k=1500)),\n",
    "    ('Linear SVM', LinearSVC(C=1))])\n",
    "\n",
    "scores = cross_val_score(pipeline, X_train_raw, Y_train, cv=folds)\n",
    "\n",
    "print(\"Stratified 5-fold CV Mean Accuracy:\", scores.mean())\n",
    "print(scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lg_svm_search' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/2s/c_l3shjj42xd90bmqxqzmjlr0000gn/T/ipykernel_27990/931725802.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# fitting the model for grid search\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mlg_svm_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_tfidf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'lg_svm_search' is not defined"
     ]
    }
   ],
   "source": [
    "# gridsearch for the best logistic regression parameters \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# defining parameter range\n",
    "grid = {'C': [0.01, 0.05, 0.1, 0.5, 1.0, 5.0, 10.0]}\n",
    " \n",
    "lg_search = GridSearchCV(LogisticRegression(max_iter=10000), grid, cv=5)\n",
    "\n",
    "# fitting the model for grid search\n",
    "lg_search.fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print best parameter after tuning\n",
    "print(lg_search.best_params_)\n",
    " \n",
    "# print how our model looks after hyper-parameter tuning\n",
    "print(lg_search.best_estimator_)\n",
    "print(\"accuracy :\", lg_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.7554660822588043\n",
      "Holdout Accuracy: 0.6785878037597433\n",
      "['negative' 'neutral' 'neutral' ... 'negative' 'neutral' 'neutral']\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.61      0.29      0.40       354\n",
      "     neutral       0.68      0.87      0.77      1273\n",
      "    positive       0.68      0.49      0.57       554\n",
      "\n",
      "    accuracy                           0.68      2181\n",
      "   macro avg       0.66      0.55      0.58      2181\n",
      "weighted avg       0.67      0.68      0.66      2181\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logreg = LogisticRegression(C=1, max_iter=5000)\n",
    "logreg.fit(X_train_tfidf, y_train)\n",
    "\n",
    "print(\"Training Accuracy:\", logreg.score(X_train_tfidf, y_train))\n",
    "print(\"Holdout Accuracy:\", logreg.score(X_dev_tfidf, y_dev))\n",
    "logreg_y_pred = logreg.predict(X_dev_tfidf)\n",
    "\n",
    "print(logreg_y_pred)\n",
    "print(classification_report(y_dev, logreg_y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5-fold CV Mean Accuracy: 0.6658093211514066\n",
      "[0.68631048 0.67048842 0.67568807 0.64243119 0.65412844]\n"
     ]
    }
   ],
   "source": [
    "# 5-fold cross validation\n",
    "# pipeline logistic regression\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('TfidfVectorizer', TfidfVectorizer(stop_words='english', min_df=5, max_df=0.8, ngram_range=(1, 3))),\n",
    "    ('Chi',  SelectKBest(chi2, k=1500)),\n",
    "    ('Logistic Regression', LogisticRegression(C=1, max_iter=5000))])\n",
    "\n",
    "scores = cross_val_score(pipeline, X_train_raw, Y_train, cv=folds)\n",
    "\n",
    "print(\"Stratified 5-fold CV Mean Accuracy:\", scores.mean())\n",
    "print(scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.6197441516742266\n",
      "Holdout Accuracy: 0.6203576341127923\n",
      "['neutral' 'positive' 'neutral' ... 'neutral' 'neutral' 'neutral']\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.60      0.09      0.15       354\n",
      "     neutral       0.61      0.95      0.75      1273\n",
      "    positive       0.70      0.19      0.30       554\n",
      "\n",
      "    accuracy                           0.62      2181\n",
      "   macro avg       0.64      0.41      0.40      2181\n",
      "weighted avg       0.63      0.62      0.54      2181\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=77)\n",
    "knn.fit(X_train_tfidf, y_train)\n",
    "\n",
    "print(\"Training Accuracy:\", knn.score(X_train_tfidf, y_train))\n",
    "print(\"Holdout Accuracy:\", knn.score(X_dev_tfidf, y_dev))\n",
    "knn_y_pred = knn.predict(X_dev_tfidf)\n",
    "\n",
    "print(knn_y_pred)\n",
    "print(classification_report(y_dev, knn_y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stratified 5-fold Cross Validation Averaged Accuracy: 0.5889826001527299\n",
      "[0.58977299 0.58954368 0.58922018 0.58692661 0.58944954]\n"
     ]
    }
   ],
   "source": [
    "# 5-fold cross validation\n",
    "# pipeline KNN\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('TfidfVectorizer', TfidfVectorizer(stop_words='english', min_df=5, max_df=0.8, ngram_range=(1, 3))),\n",
    "    ('Chi',  SelectKBest(chi2, k=1500)),\n",
    "    ('KNN', KNeighborsClassifier(n_neighbors=77))])\n",
    "\n",
    "scores = cross_val_score(pipeline, X_train_raw, Y_train, cv=folds)\n",
    "\n",
    "print(\"Stratified 5-fold CV Mean Accuracy:\", scores.mean())\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.8919524998725855\n",
      "Holdout Accuracy: 0.6882164144887666\n"
     ]
    }
   ],
   "source": [
    "# stacking\n",
    "\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "models = [\n",
    "    ('MNB', MultinomialNB()),\n",
    "    ('Logistic', LogisticRegression(C=1, max_iter=5000)),\n",
    "    ('SVC', LinearSVC(C=1)),\n",
    "    #('KNN', KNeighborsClassifier(n_neighbors=77)),\n",
    "    ('RF', RandomForestClassifier(n_estimators=100))]\n",
    "\n",
    "classifier = StackingClassifier(estimators=models)\n",
    "\n",
    "classifier.fit(X_train_tfidf, y_train)\n",
    "print('Training Accuracy:', classifier.score(X_train_tfidf, y_train))\n",
    "print('Holdout Accuracy:', classifier.score(X_dev_tfidf, y_dev))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stratified 5-fold CV Mean Accuracy: 0.6642044161237323\n",
      "[0.6785141  0.6652144  0.6766055  0.64013761 0.66055046]\n"
     ]
    }
   ],
   "source": [
    "# 5-fold cross validation\n",
    "# pipeline stacking\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('TfidfVectorizer', TfidfVectorizer(stop_words='english', min_df=5, max_df=0.8, ngram_range=(1, 3))),\n",
    "    ('Chi',  SelectKBest(chi2, k=1500)),\n",
    "    ('Stacking', classifier)])\n",
    "\n",
    "scores = cross_val_score(pipeline, X_train_raw, Y_train, cv=folds)\n",
    "\n",
    "print(\"Stratified 5-fold CV Mean Accuracy:\", scores.mean())\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>neutral</th>\n",
       "      <td>neutral</td>\n",
       "      <td>4097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>positive</th>\n",
       "      <td>positive</td>\n",
       "      <td>1153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>negative</th>\n",
       "      <td>negative</td>\n",
       "      <td>849</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             label  count\n",
       "neutral    neutral   4097\n",
       "positive  positive   1153\n",
       "negative  negative    849"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create dataframe and save as csv using id and sentiments prediction of test set\n",
    "\n",
    "def predict(train, raw_test, test_tfidf):\n",
    "\n",
    "    y_pred = classifier.predict(X_test_tfidf)\n",
    "    id_list = raw_test['id'].tolist()\n",
    "    \n",
    "    output = pd.DataFrame({'id': id_list, 'sentiment': y_pred})\n",
    "    output.to_csv('test_sentiment.csv', index=False)\n",
    "    \n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "predict(train_data, test_data, X_test_tfidf)\n",
    "\n",
    "\n",
    "test = pd.read_csv(\"test_sentiment.csv\", sep=',')\n",
    "dd = {'label': test['sentiment'].value_counts().index, 'count':test['sentiment'].value_counts()}\n",
    "pd.DataFrame(dd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
